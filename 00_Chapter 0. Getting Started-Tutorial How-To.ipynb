{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 0. Getting Started-Tutorial How-To**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오픈AI API 키 발급 및 주요 구성 요소 이해와 기본적인 기능을 시연하기 위한 간단한 \"Hello, World!\" 예제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "- Use `Ctrl + F` (Windows) or `Cmd + F` (Mac) to locate specific sections by title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Setting** \n",
    "\n",
    "    - OPENAI_API_KEY  \n",
    "\n",
    "    - How to run Solar Pro  \n",
    "\n",
    "\n",
    "- **Alternative Options for the Task**  \n",
    "\n",
    "    - Request the API with the Example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OPENAI_API_KEY**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음 링크 참고하여 키 발급 : https://wikidocs.net/233342\n",
    "- 키 발급 후 .env 파일에 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to run Solar Pro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install the required dependencies by running the following command(`Shift(or Cmd) + Enter`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'OPENAI_API_KEY' (str)\n"
     ]
    }
   ],
   "source": [
    "import pickleshare\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# To load the OPENAI_API_KEY as a variable across all notebooks in this cookbook.\n",
    "%store OPENAI_API_KEY \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = OPENAI_API_KEY,\n",
    "    base_url = \"https://api.openai.com/v1\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model= 'gpt-4o-mini',\n",
    "    messages= [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi, how are you?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Alternative Options for the Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Request the API with the Example Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_upstage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LangChain\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_upstage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatUpstage\n\u001b[1;32m      4\u001b[0m client \u001b[38;5;241m=\u001b[39m ChatUpstage(api_key\u001b[38;5;241m=\u001b[39mOPENAI_API_KEY, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_upstage'"
     ]
    }
   ],
   "source": [
    "# LangChain\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "client = ChatUpstage(api_key=OPENAI_API_KEY, model='gpt-4o-mini')\n",
    "\n",
    "response = client.invoke(\"Hi, how are you?\")\n",
    "print(response.content, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex\n",
    "from llama_index.llms.upstage import Upstage\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "client = Upstage(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    "\n",
    "message = []\n",
    "message.append(ChatMessage(role=\"user\", content=\"Hi, how are you?\"))\n",
    "\n",
    "response = client.chat(messages=message)\n",
    "print(response.message.content, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upstage LangChain integrations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| API | Description | Import | Example Usage |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "|  Chat | Build assistants using Solar Mini Chat | `from langchain_upstage import ChatUpstage` | [Go](https://python.langchain.com/v0.2/docs/integrations/chat/upstage/) |\n",
    "| Text Embedding | Embed strings to vectors | `from langchain_upstage import UpstageEmbeddings` | [Go](https://python.langchain.com/v0.2/docs/integrations/text_embedding/upstage/) |\n",
    "| Groundedness Check | Verify groundedness of assistant's response | `from langchain_upstage import UpstageGroundednessCheck` | [Go](https://python.langchain.com/v0.2/docs/integrations/tools/upstage_groundedness_check/) |\n",
    "| Layout Analysis | Serialize documents with tables and figures | `from langchain_upstage import UpstageLayoutAnalysisLoader` | [Go](https://python.langchain.com/v0.2/docs/integrations/document_loaders/upstage/) |\n",
    "\n",
    "See [documentations](https://python.langchain.com/v0.2/docs/integrations/providers/upstage/) for more details about the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick hello Langchain Upstage\n",
    "# Make sure you have the langchain-upstage package installed\n",
    "# Make sure OPENAI_API_KEY is set in the environment variables\n",
    "import os\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "llm = ChatUpstage(model=\"solar-1-mini-chat\")\n",
    "print(llm.invoke(\"Hello Mini!\"), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "llm = ChatUpstage(model=\"solar-1-mini-chat-ja\")\n",
    "print(llm.invoke(\"こんにちは、ソーラー！\"), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solar Groundedness Check\n",
    "from langchain_upstage import UpstageGroundednessCheck\n",
    "\n",
    "groundedness_check = UpstageGroundednessCheck()\n",
    "\n",
    "context = \"Solar is a language model that can generate human-like text. It is developed by Upstage.\"\n",
    "print(groundedness_check.invoke(\n",
    "    {\"context\": context, \"answer\": \"Solar is created by Upstage.\"}\n",
    "), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(groundedness_check.invoke({\"context\": context, \"answer\": \"Solar is created by OpenAI.\"}), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "embeddings_model = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n",
    "embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"What is the best season to visit Korea?\",\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
